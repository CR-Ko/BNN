{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Stochastic Neurons in Tensorflow\n",
    "\n",
    "###### A notebook and blog post by [R2RT](http://r2rt.com/)\n",
    "\n",
    "In this post, I introduce and discuss binary stochastic neurons, implement trainable binary stochastic neurons in Tensorflow, and conduct several simple experiments on the MNIST dataset to get a feel for their behavior. Binary stochastic neurons offer two advantages over real-valued neurons: they can act as a regularizer and they enable conditional computation by enabling a network to make yes/no decisions. Conditional computation opens the door to new and exciting neural network architectures, such as the choice of experts architecture and heirarchical multiscale neural networks, which I plan to discuss in future posts.\n",
    "\n",
    "### The binary stochastic neuron\n",
    "\n",
    "A binary stochastic neuron is a neuron with a noisy output: some proportion $p$ of the time it outputs 1, otherwise 0. An easy way to turn a real-valued input, $a$, into this proportion, $p$, is to set $p = \\text{sigm}(a)$, where $\\text{sigm}$ is the logistic sigmoid, $\\text{sigm}(x) = \\frac{1}{1 + \\exp(-x)}$. Thus, we define the binary stochastic neuron, $\\text{BSN}$, as:\n",
    "\n",
    "$$\\text{BSN}(a) = \\textbf{1}_{z\\ \\lt\\ \\text{sigm}(a)}$$\n",
    "\n",
    "where $\\textbf{1}_{x}$ is the [indicator function](https://en.wikipedia.org/wiki/Indicator_function) on the truth value of $x$ and $z \\sim U[0,1]$.\n",
    "\n",
    "\n",
    "### Advantages of the binary stochastic neuron\n",
    "\n",
    "1. A binary stochastic neuron is a noisy modification of the logistic sigmoid: instead of outputting $p$, it outputs 1 with probability $p$ and 0 otherwise. Noise generally serves as a regularizer (see, e.g., [Srivastava et al. (2014)](http://www.jmlr.org/papers/v15/srivastava14a.html) and [Neelakantan et al. (2015)](https://arxiv.org/abs/1511.06807)), and so we might expect the same from binary stochastic neurons as compared to the logistic neurons. Indeed, this is the claimed \"unpublished result\" from the end of [Hinton et al.'s Coursera Lecture 9c](https://www.youtube.com/watch?v=LN0xtUuJsEI&list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&index=41), which I test empirically in this post. Unfortunately, the results below show that binary stochastic neurons do not work so well as regularizers on the MNIST dataset, though they may serve as viable regularizers in other cases.\n",
    "\n",
    "2. More importantly, by enabling networks to make binary decisions, the binary stochastic neuron allows for conditional computation. This opens the door to some interesting new architectures. For example, instead of a mixture of experts architecture, which weights the outputs of several \"expert\" sub-networks and requires that all subnetworks be computed, we could use a *choice* of experts architecture, which conditionally uses expert sub-networks as needed. This architecture is implicitly proposed in [Bengio et al. (2013)](https://arxiv.org/abs/1308.3432), wherein the experiments use a choice of expert units architecture (i.e., a gated architecture where gates must be 1 or 0). Another example, proposed in [Bengio et al. (2013)](https://arxiv.org/abs/1308.3432) and implemented by [Chung et al. (2016)](https://arxiv.org/abs/1609.01704), is the Heirarchical Multiscale Recurrent Neural Network (HM-RNN) architecture, which achieves great results on language modelling tasks. Both of these architectures will be explored in future posts.\n",
    "\n",
    "### Training the binary stochastic neuron\n",
    "\n",
    "For any single trial, the binary stochastic neuron generally has a derivative of 0 and cannot be trained by simple backpropagation. To see this, consider that if $z \\neq \\text{sigm}(a)$ in the $\\text{BSN}$ function above, there exists a [neighborhood](https://en.wikipedia.org/wiki/Neighbourhood_(mathematics)) around $a$ such that the output of $\\text{BSN}(a)$ is unchanged (i.e., the derivative is 0). We get around this by *estimating* the derivative with respect to the *expected* loss, rather than calculating the derivative with respect to the outcome of a single trial. We can only estimate this derivative, because in any given trial, we only see the loss value with respect to the given noise -- we don't know what the loss would have been given another level of noise. We call a method that provides such an estimate an \"estimator\". An estimator is *unbiased* if the expectation of its estimate equals the expectation of the derivative it is estimating; otherwise, it is *biased*.\n",
    "\n",
    "In this post we implement the two estimators discussed in [Bengio et al. (2013)](https://arxiv.org/abs/1308.3432):\n",
    "\n",
    "1. The REINFORCE estimator, which is an unbiased estimator and a special case of the REINFORCE algorithm discussed in [Williams (1992)](http://link.springer.com/article/10.1007/BF00992696).\n",
    "\n",
    "    The REINFORCE estimator estimates the expectation of $\\frac{\\partial L}{\\partial a}$ as $(\\text{BSN}(a) - \\text{sigm}(a))(L - c)$, where $c$ is a constant. [Bengio et al. (2013)](https://arxiv.org/abs/1308.3432) proves that:\n",
    "\n",
    "    $$\\mathbb{E}[(\\text{BSN}(a) - \\text{sigm}(a))(L - c)] = \\mathbb{E}\\big[\\frac{\\partial L}{\\partial a}\\big].$$\n",
    "\n",
    "    [Bengio et al. (2013)](https://arxiv.org/abs/1308.3432) further shows that to minimize the variance of the estimation, we choose:\n",
    "\n",
    "    $$c = \\bar L = \\frac{\\mathbb{E}[\\text{BSN}(a) - \\text{sigm}(a))^2L]}{\\mathbb{E}[\\text{BSN}(a) - \\text{sigm}(a))^2]}$$\n",
    "\n",
    "    which we can practically implement by keeping track of the numerator and denominator as a moving average. Interestingly, the REINFORCE estimator does not require any backpropagated loss gradient--it operates directly on the loss of the network.\n",
    "\n",
    "2. The straight through (ST) estimator, which is a biased estimator that was first proposed by [Hinton et al.'s Coursera Lecture 9c](https://www.youtube.com/watch?v=LN0xtUuJsEI&list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&index=41).\n",
    "\n",
    "    The ST estimator simply replaces the derivative factor used during backpropagation, $\\frac{d\\text{BSN}(a)}{da} = 0$, with $\\frac{d\\text{BSN}(a)}{da} = \\text{BSN}(a)$. A variant of the ST estimator replaces the derivative factor with $\\frac{d\\text{BSN}(a)}{da} = \\text{sigm}(a)$. Whereas [Bengio et al. (2013)](https://arxiv.org/abs/1308.3432) found that the former is more effective, the latter variant was successfully used in [Chung et al. (2016)](https://arxiv.org/abs/1609.01704) in combination with the *slope-annealing trick* and deterministic binary neurons (which we will see perform very similarly, if not better, than stochastic binary neurons when used with slope-annealing). The slope-anealing trick modifies $\\text{BSN}(a)$ by first multiplying the input $a$ by a slope $m$ as follows:\n",
    "\n",
    "    $$\\text{BSN}_{\\text{SL}(m)}(a) = \\textbf{1}_{z \\gt \\text{sigm}(ma)}.$$\n",
    "\n",
    "    Then, we increase the slope as training progresses and use $\\frac{d\\text{BSN}(a)}{da} = \\text{sigm}(ma)$ when computing the gradient. The idea behind this is that as the slope increases, the logistic sigmoid approaches a step function, so that it's derivative approaches the true derivative. All three variants are tested in this post.\n",
    "\n",
    "### Implementing the binary stochastic neuron in Tensorflow\n",
    "\n",
    "The tricky part of implementing a binary stochastic neuron in Tensorflow is not the forward computation, but the implementation of the REINFORCE and straight through estimators. Each requires replacing the gradient of one or more Tensorflow operations. The [official approach](https://www.tensorflow.org/versions/r0.10/how_tos/adding_an_op/index.html) to this is to write a new op in C++, which seems wholly unnecessary. There are, however, two workable unofficial approaches, one of which is [a trick credited to Sergey Ioffe](http://stackoverflow.com/questions/36456436/how-can-i-define-only-the-gradient-for-a-tensorflow-subgraph/36480182), and another that uses `gradient_override_map`, an experimental feature of Tensorflow that is documented [here](https://www.tensorflow.org/versions/r0.10/api_docs/python/framework.html). We will use `gradient_override_map`, which works well for our purposes.\n",
    "\n",
    "#### Imports and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "from tensorflow.python.framework import ops\n",
    "from enum import Enum\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "def layer_linear(inputs, shape, scope='linear_layer'):\n",
    "    with tf.variable_scope(scope):\n",
    "        w = tf.get_variable('w',shape)\n",
    "        b = tf.get_variable('b',shape[-1:])\n",
    "    return tf.matmul(inputs,w) + b\n",
    "\n",
    "def layer_softmax(inputs, shape, scope='softmax_layer'):\n",
    "    with tf.variable_scope(scope):\n",
    "        w = tf.get_variable('w',shape)\n",
    "        b = tf.get_variable('b',shape[-1:])\n",
    "    return tf.nn.softmax(tf.matmul(inputs,w) + b)\n",
    "\n",
    "def accuracy(y, pred):\n",
    "    correct = tf.equal(tf.argmax(y,1), tf.argmax(pred,1))\n",
    "    return tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "def plot_n(data_and_labels, lower_y = 0., title=\"Learning Curves\"):\n",
    "    fig, ax = plt.subplots()\n",
    "    for data, label in data_and_labels:\n",
    "        ax.plot(range(0,len(data)*100,100),data, label=label)\n",
    "    ax.set_xlabel('Training steps')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_ylim([lower_y,1])\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc=4)\n",
    "    plt.show()\n",
    "    \n",
    "class StochasticGradientEstimator(Enum):\n",
    "    ST = 0\n",
    "    REINFORCE = 1   \n",
    "    \n",
    "    \n",
    "    \n",
    "print('OK!!!!')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary stochastic neuron with straight through estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ops' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f94aa0c9b818>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegisterGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BinaryRound\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_binaryRound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;34m\"\"\"Straight through estimator for the binaryRound op (identity if 1, else 0).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ops' is not defined"
     ]
    }
   ],
   "source": [
    "def binaryRound(x):\n",
    "    \"\"\"\n",
    "    Rounds a tensor whose values are in [0,1] to a tensor with values in {0, 1}, \n",
    "    using the straight through estimator for the gradient.\n",
    "    \n",
    "    E.g.,:\n",
    "    If x is >= 0.5, binaryRound(x) will be 1 and the gradient will be pass-through,\n",
    "    otherwise, binaryRound(x) will be 0 and the gradient will be 0.\n",
    "    \"\"\"\n",
    "    g = tf.get_default_graph()\n",
    "    \n",
    "    with ops.name_scope(\"BinaryRound\") as name:\n",
    "        # override \"Floor\" because tf.round uses tf.floor\n",
    "        with g.gradient_override_map({\"Floor\": \"BinaryRound\"}):\n",
    "            return tf.round(x, name=name)\n",
    "        \n",
    "@ops.RegisterGradient(\"BinaryRound\")\n",
    "def _binaryRound(op, grad):\n",
    "    \"\"\"Straight through estimator for the binaryRound op (identity if 1, else 0).\"\"\"\n",
    "    x = op.outputs[0]\n",
    "    return x * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bernoulliSample(x):\n",
    "    \"\"\"\n",
    "    Uses a tensor whose values are in [0,1] to sample a tensor with values in {0, 1},\n",
    "    using the straight through estimator for the gradient.\n",
    "    \n",
    "    E.g.,:\n",
    "    if x is 0.6, bernoulliSample(x) will be 1 with probability 0.6, and 0 otherwise,\n",
    "    and the gradient will be pass-through (1) wih probability 0.6, and 0 otherwise. \n",
    "    \"\"\"\n",
    "    g = tf.get_default_graph()\n",
    "    \n",
    "    with ops.name_scope(\"BernoulliSample\") as name:\n",
    "        with g.gradient_override_map({\"Ceil\": \"Identity\",\"Sub\": \"BernoulliSample_ST\"}):\n",
    "            return tf.ceil(x - tf.random_uniform(tf.shape(x)), name=name)\n",
    "        \n",
    "@ops.RegisterGradient(\"BernoulliSample_ST\")\n",
    "def bernoulliSample_ST(op, grad):\n",
    "    \"\"\"Straight through estimator for the bernoulliSample op (identity if 1, else 0).\"\"\"\n",
    "    sub = op.outputs[0] # x - tf.random_uniform... \n",
    "    res = sub.consumers()[0].outputs[0] # tf.ceil(sub)\n",
    "    return [res * grad, tf.zeros(tf.shape(op.inputs[1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def passThroughSigmoid(x, slope=1):\n",
    "    \"\"\"Sigmoid that uses identity function as its gradient\"\"\"\n",
    "    g = tf.get_default_graph()\n",
    "    with ops.name_scope(\"PassThroughSigmoid\") as name:\n",
    "        with g.gradient_override_map({\"Sigmoid\": \"Identity\"}):\n",
    "            return tf.sigmoid(x, name=name)\n",
    "        \n",
    "def binaryStochastic_ST(x, slope_tensor=None, pass_through=True, stochastic=True):\n",
    "    \"\"\"\n",
    "    Sigmoid followed by either a random sample from a bernoulli distribution according \n",
    "    to the result (binary stochastic neuron) (default), or a sigmoid followed by a binary\n",
    "    step function (if stochastic == False). Uses the straight through estimator. \n",
    "    See https://arxiv.org/abs/1308.3432.\n",
    "    \n",
    "    Arguments:\n",
    "    * x: the pre-activation / logit tensor\n",
    "    * slope_tensor: if passThrough==False, slope adjusts the slope of the sigmoid function \n",
    "        for purposes of the Slope Annealing Trick (see http://arxiv.org/abs/1609.01704)\n",
    "    * pass_through: if True (default), gradient of the entire function is 1 or 0; \n",
    "        if False, gradient of 1 is scaled by the gradient of the sigmoid (required if\n",
    "        Slope Annealing Trick is used)\n",
    "    * stochastic: binary stochastic neuron if True (default), or step function if False\n",
    "    \"\"\"\n",
    "    if slope_tensor is None:\n",
    "        slope_tensor = tf.constant(1.0)\n",
    "        \n",
    "    if pass_through:\n",
    "        p = passThroughSigmoid(x)\n",
    "    else:\n",
    "        p = tf.sigmoid(slope_tensor*x)\n",
    "    \n",
    "    if stochastic:\n",
    "        return bernoulliSample(p)\n",
    "    else:\n",
    "        return binaryRound(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary stochastic neuron with REINFORCE estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binaryStochastic_REINFORCE(x, stochastic = True, loss_op_name=\"loss_by_example\"):\n",
    "    \"\"\"\n",
    "    Sigmoid followed by a random sample from a bernoulli distribution according\n",
    "    to the result (binary stochastic neuron). Uses the REINFORCE estimator.\n",
    "    See https://arxiv.org/abs/1308.3432.\n",
    "    \n",
    "    NOTE: Requires a loss operation with name matching the argument for loss_op_name \n",
    "    in the graph. This loss operation should be broken out by example (i.e., not a\n",
    "    single number for the entire batch).\n",
    "    \"\"\"\n",
    "    g = tf.get_default_graph()\n",
    "\n",
    "    with ops.name_scope(\"BinaryStochasticREINFORCE\"):    \n",
    "        with g.gradient_override_map({\"Sigmoid\": \"BinaryStochastic_REINFORCE\", \n",
    "                                      \"Ceil\": \"Identity\"}):\n",
    "            p = tf.sigmoid(x)\n",
    "            \n",
    "            reinforce_collection = g.get_collection(\"REINFORCE\")\n",
    "            if not reinforce_collection:\n",
    "                g.add_to_collection(\"REINFORCE\", {})\n",
    "                reinforce_collection = g.get_collection(\"REINFORCE\")\n",
    "            reinforce_collection[0][p.op.name] = loss_op_name\n",
    "            \n",
    "            return tf.ceil(p - tf.random_uniform(tf.shape(x)))\n",
    "\n",
    "            \n",
    "@ops.RegisterGradient(\"BinaryStochastic_REINFORCE\")\n",
    "def _binaryStochastic_REINFORCE(op, _):\n",
    "    \"\"\"Unbiased estimator for binary stochastic function based on REINFORCE.\"\"\"\n",
    "    loss_op_name = op.graph.get_collection(\"REINFORCE\")[0][op.name]\n",
    "    loss_tensor = op.graph.get_operation_by_name(loss_op_name).outputs[0]\n",
    "    \n",
    "    sub_tensor = op.outputs[0].consumers()[0].outputs[0] #subtraction tensor\n",
    "    ceil_tensor = sub_tensor.consumers()[0].outputs[0] #ceiling tensor\n",
    "    \n",
    "    outcome_diff = (ceil_tensor - op.outputs[0])\n",
    "    \n",
    "    # Provides an early out if we want to avoid variance adjustment for\n",
    "    # whatever reason (e.g., to show that variance adjustment helps)\n",
    "    if op.graph.get_collection(\"REINFORCE\")[0].get(\"no_variance_adj\"):\n",
    "        return outcome_diff * tf.expand_dims(loss_tensor, 1)\n",
    "    \n",
    "    outcome_diff_sq = tf.square(outcome_diff)\n",
    "    outcome_diff_sq_r = tf.reduce_mean(outcome_diff_sq, reduction_indices=0)\n",
    "    outcome_diff_sq_loss_r = tf.reduce_mean(outcome_diff_sq * tf.expand_dims(loss_tensor, 1),\n",
    "                                            reduction_indices=0)\n",
    "    \n",
    "    L_bar_num = tf.Variable(tf.zeros(outcome_diff_sq_r.get_shape()), trainable=False)\n",
    "    L_bar_den = tf.Variable(tf.ones(outcome_diff_sq_r.get_shape()), trainable=False)\n",
    "    \n",
    "    #Note: we already get a decent estimate of the average from the minibatch\n",
    "    decay = 0.95 \n",
    "    train_L_bar_num = tf.assign(L_bar_num, L_bar_num*decay +\\\n",
    "                                            outcome_diff_sq_loss_r*(1-decay))\n",
    "    train_L_bar_den = tf.assign(L_bar_den, L_bar_den*decay +\\\n",
    "                                            outcome_diff_sq_r*(1-decay))\n",
    "\n",
    "\n",
    "    with tf.control_dependencies([train_L_bar_num, train_L_bar_den]):   \n",
    "        L_bar = train_L_bar_num/(train_L_bar_den+1e-4)\n",
    "        L = tf.tile(tf.expand_dims(loss_tensor,1),\n",
    "                    tf.constant([1,L_bar.get_shape().as_list()[0]]))\n",
    "        return outcome_diff * (L - L_bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapper to create layer of binary stochastic neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_wrapper(\\\n",
    "                pre_activations_tensor,\n",
    "                estimator=StochasticGradientEstimator.ST,\n",
    "                stochastic_tensor=tf.constant(True), \n",
    "                pass_through=True, \n",
    "                slope_tensor=tf.constant(1.0)):\n",
    "    \"\"\"\n",
    "    Turns a layer of pre-activations (logits) into a layer of binary stochastic neurons\n",
    "    \n",
    "    Keyword arguments:\n",
    "    *estimator: either ST or REINFORCE\n",
    "    *stochastic_tensor: a boolean tensor indicating whether to sample from a bernoulli \n",
    "        distribution (True, default) or use a step_function (e.g., for inference)\n",
    "    *pass_through: for ST only - boolean as to whether to substitute identity derivative on the \n",
    "        backprop (True, default), or whether to use the derivative of the sigmoid\n",
    "    *slope_tensor: for ST only - tensor specifying the slope for purposes of slope annealing\n",
    "        trick\n",
    "    \"\"\"\n",
    "    \n",
    "    if estimator == StochasticGradientEstimator.ST:\n",
    "        if pass_through:\n",
    "            return tf.cond(stochastic_tensor, \n",
    "                    lambda: binaryStochastic_ST(pre_activations_tensor), \n",
    "                    lambda: binaryStochastic_ST(pre_activations_tensor, stochastic=False))  \n",
    "        else:\n",
    "            return tf.cond(stochastic_tensor, \n",
    "                    lambda: binaryStochastic_ST(pre_activations_tensor, slope_tensor = slope_tensor, \n",
    "                                             pass_through=False), \n",
    "                    lambda: binaryStochastic_ST(pre_activations_tensor, slope_tensor = slope_tensor, \n",
    "                                             pass_through=False, stochastic=False))\n",
    "    elif estimator == StochasticGradientEstimator.REINFORCE:\n",
    "        # binaryStochastic_REINFORCE was designed to only be stochastic, so using the ST version\n",
    "        # for the step fn for purposes of using step fn at evaluation / not for training\n",
    "        return tf.cond(stochastic_tensor,\n",
    "                lambda: binaryStochastic_REINFORCE(pre_activations_tensor),\n",
    "                lambda: binaryStochastic_ST(pre_activations_tensor, stochastic=False))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized estimator.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to build graph for MNIST classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_classifier(hidden_dims=[100], \n",
    "                        lr = 0.5, \n",
    "                        pass_through = True, \n",
    "                        non_binary = False, \n",
    "                        estimator = StochasticGradientEstimator.ST,\n",
    "                        no_var_adj=False):\n",
    "    reset_graph()\n",
    "    g = {}\n",
    "    \n",
    "    if no_var_adj:\n",
    "        tf.get_default_graph().add_to_collection(\"REINFORCE\", {\"no_variance_adj\": no_var_adj})\n",
    "\n",
    "    g['x'] = tf.placeholder(tf.float32, [None, 784], name='x_placeholder')\n",
    "    g['y'] = tf.placeholder(tf.float32, [None, 10], name='y_placeholder')\n",
    "    g['stochastic'] = tf.constant(True)\n",
    "    g['slope'] = tf.constant(1.0)\n",
    "    \n",
    "    g['layers'] = {0: g['x']}\n",
    "    hidden_layers = len(hidden_dims)\n",
    "    dims = [784] + hidden_dims\n",
    "    \n",
    "    for i in range(1, hidden_layers+1):\n",
    "        with tf.variable_scope(\"layer_\" + str(i)):\n",
    "            pre_activations = layer_linear(g['layers'][i-1], dims[i-1:i+1], scope='layer_' + str(i))\n",
    "            if non_binary:\n",
    "                g['layers'][i] = tf.sigmoid(pre_activations)\n",
    "            else:\n",
    "                g['layers'][i] = binary_wrapper(pre_activations, \n",
    "                                              estimator = estimator,\n",
    "                                              pass_through = pass_through, \n",
    "                                              stochastic_tensor = g['stochastic'], \n",
    "                                              slope_tensor = g['slope'])\n",
    "    \n",
    "    g['pred'] = layer_softmax(g['layers'][hidden_layers], [dims[-1], 10])\n",
    "    \n",
    "    g['loss'] = -tf.reduce_mean(g['y'] * tf.log(g['pred']),reduction_indices=1)\n",
    "    \n",
    "    # named loss_by_example necessary for REINFORCE estimator\n",
    "    tf.identity(g['loss'], name=\"loss_by_example\") \n",
    "    \n",
    "    g['ts'] = tf.train.GradientDescentOptimizer(lr).minimize(g['loss'])\n",
    "\n",
    "    g['accuracy'] = accuracy(g['y'], g['pred'])\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_classifier(\\\n",
    "        hidden_dims=[100,100],\n",
    "        estimator=StochasticGradientEstimator.ST,\n",
    "        stochastic_train=True, \n",
    "        stochastic_eval=True, \n",
    "        slope_annealing_rate=None, \n",
    "        epochs=10, \n",
    "        lr=0.5,\n",
    "        non_binary=False,\n",
    "        no_var_adj=False,\n",
    "        train_set = mnist.train,\n",
    "        val_set = mnist.validation,\n",
    "        verbose=False,\n",
    "        label=None):\n",
    "    if slope_annealing_rate is None:\n",
    "        g = build_classifier(hidden_dims=hidden_dims, lr=lr, pass_through=True, \n",
    "                                non_binary=non_binary, estimator=estimator, no_var_adj=no_var_adj)\n",
    "    else:\n",
    "        g = build_classifier(hidden_dims=hidden_dims, lr=lr, pass_through=False, \n",
    "                                non_binary=non_binary, estimator=estimator, no_var_adj=no_var_adj)\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        slope = 1\n",
    "        res_tr, res_val = [], []\n",
    "        for epoch in range(epochs):        \n",
    "            feed_dict={g['x']: val_set.images, \n",
    "                       g['y']: val_set.labels, \n",
    "                       g['stochastic']: stochastic_eval,\n",
    "                       g['slope']: slope}\n",
    "            if verbose:\n",
    "                print(\"Epoch\", epoch, sess.run(g['accuracy'], feed_dict=feed_dict))\n",
    "\n",
    "            accuracy = 0\n",
    "            for i in range(1001):\n",
    "                x, y = train_set.next_batch(50)\n",
    "                feed_dict={g['x']: x, g['y']: y, g['stochastic']: stochastic_train}\n",
    "                acc, _ = sess.run([g['accuracy'],g['ts']], feed_dict=feed_dict)\n",
    "                accuracy += acc\n",
    "                if i % 100 == 0 and i > 0:\n",
    "                    res_tr.append(accuracy/100)\n",
    "                    accuracy = 0\n",
    "                    feed_dict={g['x']: val_set.images, \n",
    "                               g['y']: val_set.labels, \n",
    "                               g['stochastic']: stochastic_eval,\n",
    "                               g['slope']: slope}\n",
    "                    res_val.append(sess.run(g['accuracy'], feed_dict=feed_dict))                \n",
    "\n",
    "            if slope_annealing_rate is not None:\n",
    "                slope = slope*slope_annealing_rate\n",
    "                if verbose:\n",
    "                    print(\"Sigmoid slope:\", slope)\n",
    "\n",
    "        feed_dict={g['x']: val_set.images, g['y']: val_set.labels, \n",
    "                   g['stochastic']: stochastic_eval, g['slope']: slope}\n",
    "        print(\"Epoch\", epoch+1, sess.run(g['accuracy'], feed_dict=feed_dict))\n",
    "        if label is not None:\n",
    "            return (res_tr, label + \" - Training\"), (res_val, label + \" - Validation\")\n",
    "        else:\n",
    "            return [(res_tr, \"Training\"), (res_val, \"Validation\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "We've now set up a good foundation from which we can run a number of simple experiments. The experiments are as follows:\n",
    "\n",
    "- **Experiment 0**: A non-stochastic, non-binary baseline.\n",
    "- **Experiment 1**: A comparison of variance-adjusted REINFORCE and non-variance adjusted REINFORCE, which shows that the variance adjustment allows for faster learning and higher learning rates.\n",
    "- **Experiment 2**: A comparison of pass-through ST and sigmoid-adjusted ST, which shows that the pass-through ST estimator obtains better results, in accordance with the findings of [Bengio et al. (2013](https://arxiv.org/abs/1308.3432). \n",
    "- **Experiment 3**: A comparison of pass-through ST and slope-annealed sigmoid-adjusted ST, which shows that a well-tuned slope-annealed ST performs slightly better than the pass-through ST.\n",
    "- **Experiment 4**: A direct comparison of variance-adjusted REINFORCE and slope-annealed ST, which shows that ST performs significantly better than REINFORCE.\n",
    "- **Experiment 5**: A look at the deterministic step function, during training and evaluation, which shows that in the absence of slope annealing, stochasticity is necessary during training, but that deterministic evaluation can provide a slight boost at inference, and that with slope annealing, deterministic training is just as effective, if not more effective than stochastic training.\n",
    "- **Experiment 6**: A look at how network depth affects performance, which shows that deep stochastic networks are difficult to train. \n",
    "- **Experiment 7**: A look at using binary stochastic neurons as a regularizer, which shows that they do function as a regularizer, but it is unclear whether it is worthwhile to use them as one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 0: A non-stochastic, non-binary baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = train_classifier(hidden_dims=[100], epochs=20, lr=1.0, non_binary=True)\n",
    "plot_n(res, lower_y=0.8, title=\"Logistic Sigmoid Baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1: Variance-adjusted vs. not variance-adjusted REINFORCE\n",
    "\n",
    "Recall that the REINFORCE estimator estimates the expectation of $\\frac{\\partial L}{\\partial a}$ as $(\\text{BSN}(a) - \\text{sigm}(a))(L - c)$, where $c$ is a constant. The non-variance-adjusted form of REINFORCE uses $c = 0$, whereas the variance-adjusted form uses the variance minimizing result stated above. Naturally we should prefer the least variance, and the experimental results below agree. \n",
    "\n",
    "It seems that both forms of REINFORCE often break down for learning rates greater than or equal to 0.3 (compare to the learning rate of 1.0 that used in Experiment 0). After a few trials, variance-adjusted REINFORCE appears to be more resistant to such failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Variance-adjusted:\")\n",
    "res1 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.REINFORCE, epochs=3, \n",
    "                       lr=0.3, verbose=True)\n",
    "print(\"Not variance-adjusted:\")\n",
    "res2= train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.REINFORCE, epochs=3, \n",
    "                       lr=0.3, no_var_adj=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of performance at lower learning rates, a learning rate of about 0.05 provided the best results. The results show that the variance-adjusted REINFORCE learns faster, but that its non-variance adjusted eventually catches up. This result is consistent with the mathematical result that they are both unbiased estimators. Performance is predictably worse than it was for the plain logistic sigmoid in Experiment 0, although there is almost no generalization gap, consistent with the hypothesis that binary stochastic neurons can act as regularizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res1 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.REINFORCE, epochs=20, \n",
    "                       lr=0.05, label = \"Variance-adjusted\")\n",
    "res2= train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.REINFORCE, epochs=20, \n",
    "                       lr=0.05, no_var_adj=True, label = \"Not variance-adjusted\")\n",
    "\n",
    "plot_n(res1 + res2, lower_y=0.6, title=\"Experiment 1: REINFORCE variance adjustment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2: Pass-through vs. sigmoid-adjusted ST estimation\n",
    "\n",
    "Recall that one variant of the straight-through estimator uses the identity function as the backpropagated gradient when the neuron emits a 1 (pass-through), and another variance multiplies that by the gradient of the logistic sigmoid that the neuron calculates (sigmoid-adjusted). In [Bengio et al. (2013)](https://arxiv.org/abs/1308.3432), it was remarked that, surprisingly, the former performs better. My results below agree; we see that the pass-through variant edges out the sigmoid-adjusted variant at higher learning rates, and interestingly, at a learning rate of 1, the sigmoid-adjusted variant performs very poorly and fails to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res1 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.ST, epochs=20, \n",
    "                       lr=0.1, label = \"Pass-through - 0.1\")\n",
    "res2 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.ST, epochs=20, \n",
    "                       lr=0.1, slope_annealing_rate = 1.0, label = \"Sigmoid-adjusted - 0.1\")\n",
    "\n",
    "res3 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.ST, epochs=20, \n",
    "                       lr=0.3, label = \"Pass-through - 0.3\")\n",
    "res4 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.ST, epochs=20, \n",
    "                       lr=0.3, slope_annealing_rate = 1.0, label = \"Sigmoid-adjusted - 0.3\")\n",
    "\n",
    "res5 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.ST, epochs=20, \n",
    "                       lr=1.0, label = \"Pass-through - 1.0\")\n",
    "res6 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.ST, epochs=20, \n",
    "                       lr=1.0, slope_annealing_rate = 1.0, label = \"Sigmoid-adjusted - 1.0\")\n",
    "\n",
    "plot_n(res1[1:] + res2[1:] + res3[1:] + res4[1:] + res5[1:] + res6[1:], \n",
    "       lower_y=0.4, title=\"Experiment 2: Pass-through vs sigmoid-adjusted ST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 3: Pass-through vs. slope-annealed ST estimation\n",
    "\n",
    "Recall that [Chung et al. (2016)](https://arxiv.org/abs/1609.01704) improves upon the sigmoid-adjusted variant of the ST estimator by using the *slope-annealing trick*, which slowly increases the slope of the logistic sigmoid as training progresses. Using the slope-annealing trick with an annealing rate of 1.1 times per epoch (so the slope at epoch 20 is $1.1^{19} \\approx 6.1$), we're able to match and even surpass the results of the pass-through ST estimator. The slope-annealed estimator is still more sensitive to the learning rate (performing similarly to the sigmoid-adjusted variant at a learning rate of 1.0), but it appears that it can be fine tuned to perform better than the pass-through variant. Note that the slope annealed neuron used here is not the same as the one used by [Chung et al. (2016)](https://arxiv.org/abs/1609.01704), who employ a deterministic step function and use a hard sigmoid in place of a sigmoid for the backpropagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res1 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.ST, epochs=20, \n",
    "                       lr=0.03, label = \"Pass-through - 0.03\")\n",
    "res2 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.ST, epochs=20, \n",
    "                       lr=0.03, slope_annealing_rate = 1.1, label = \"Slope-annealed - 0.03\")\n",
    "\n",
    "res3 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.ST, epochs=20, \n",
    "                       lr=0.1, label = \"Pass-through - 0.1\")\n",
    "res4 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.ST, epochs=20, \n",
    "                       lr=0.1, slope_annealing_rate = 1.1, label = \"Slope-annealed - 0.1\")\n",
    "\n",
    "res5 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.ST, epochs=20, \n",
    "                       lr=0.3, label = \"Pass-through - 0.3\")\n",
    "res6 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.ST, epochs=20, \n",
    "                       lr=0.3, slope_annealing_rate = 1.1, label = \"Slope-annealed - 0.3\")\n",
    "\n",
    "plot_n(res1[1:] + res2[1:] + res3[1:] + res4[1:] + res5[1:] + res6[1:],\n",
    "       lower_y=0.6, title=\"Experiment 3: Pass-through vs slope-annealed ST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 4: Variance-adjusted REINFORCE vs slope-annealed ST\n",
    "\n",
    "We now directly compare the variance-adjusted REINFORCE and slope-annealed ST, both at their best learning rates. In this setting, despite being a biased estimator, the straight-through estimator displays faster learning, less variance, and better overall results than the variance-adjusted REINFORCE estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res1 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.REINFORCE, epochs=20, \n",
    "                       lr=0.05, label = \"Variance-adjusted REINFORCE\")\n",
    "\n",
    "res2 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.ST, epochs=20, \n",
    "                       lr=0.1, slope_annealing_rate = 1.1, label = \"Slope-annealed ST\")\n",
    "\n",
    "plot_n(res1[1:] + res2[1:],\n",
    "       lower_y=0.6, title=\"Experiment 4: Variance-adjusted REINFORCE vs slope-annealed ST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 5: A look at the deterministic step function, during training and evaluation\n",
    "\n",
    "Similar to how dropout is not applied at inference when using dropout for training, it makes sense that we might replace the stochastic sigmoid with a deterministic step function at inference when using binary neurons. We might go even further than that, and use deterministic neurons during training, which is the approach taken by [Chung et al. (2016)](https://arxiv.org/abs/1609.01704). The following three combinations are compared below, using the pass-through straight through estimator, without slope annealing:\n",
    "\n",
    "- stochastic during training, stochastic during test\n",
    "- stochastic during training, deterministic during test\n",
    "- deterministic during training, deterministic during test\n",
    "\n",
    "The results show that stochastic inference and deterministic inference, when combined with stochastic training, are closely comparable (with the latter having a slight edge), but that deterministic training lags behind stochastic training. Similar results hold for the REINFORCE estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res1 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.ST, epochs=20, \n",
    "                        lr=0.3, label = \"Stochastic, Stochastic\")\n",
    "res2 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.ST, epochs=20, \n",
    "                        lr=0.3, stochastic_eval=False, label = \"Stochastic, Deterministic\")\n",
    "res3 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.ST, epochs=20, \n",
    "                        lr=0.3, stochastic_train=False, stochastic_eval=False, \n",
    "                        label = \"Deterministic, Deterministic\")\n",
    "\n",
    "plot_n(res1 + res2 + res3,\n",
    "       lower_y=0.6, title=\"Experiment 5: Stochastic vs Deterministic (Straight-through, no slope annealing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The introduction of slope annealing, however, as per [Chung et al. (2016)](https://arxiv.org/abs/1609.01704), closes the gap between stochastic and deterministic training. Although stochastic and deterministic training converge to a similar result, deterministic training results in faster training early on. Note that the slope annealed neuron used here is not exactly the same as the one used by [Chung et al. (2016)](https://arxiv.org/abs/1609.01704); this one uses a regular sigmoid, whereas Chung et al. use a hard sigmoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res1 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.ST, epochs=30, \n",
    "                        lr=0.1, slope_annealing_rate=1.1, stochastic_eval=False, \n",
    "                        label = \"Stochastic, Deterministic (Slope annealed)\")\n",
    "res2 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.ST, epochs=30, \n",
    "                        lr=0.1, slope_annealing_rate=1.1, stochastic_train=False, stochastic_eval=False, \n",
    "                        label = \"Deterministic, Deterministic (Slope annealed)\")\n",
    "\n",
    "plot_n(res1[1:] + res2[1:],\n",
    "       lower_y=0.90, title=\"Experiment 5: Stochastic vs Deterministic (Straight-through, with slope annealing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 6: The effect of depth on REINFORCE and ST estimators\n",
    "\n",
    "Next, I look at how each estimator interacts with depth. From a theoretical perpective, there is reason to think the straight-through estimator will suffer from depth; as noted by [Bengio et al. (2013)](https://arxiv.org/abs/1308.3432), it is not even guaranteed to have the same sign as the expected gradient during backpropagation. It turns out that if we keep the learning rate constant, both estimators start to fail as we increase depth. However, if we lower the learning rate dramatically (300x for the ST estimator and 25x for the REINFORCE estimator), we can start to get the deeper networks to train. In constrast with the results of earlier experiments, the bias of the straight through estimator starts to show and the REINFORCE estimator is the clear winner at higher depths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res1 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.ST, epochs=20, \n",
    "                        lr=0.3, label = \"1 hidden layer\")\n",
    "res2 = train_classifier(hidden_dims=[100, 100], estimator=StochasticGradientEstimator.ST, epochs=20, \n",
    "                        lr=0.3, label = \"2 hidden layers\")\n",
    "res3 = train_classifier(hidden_dims=[100, 100, 100], estimator=StochasticGradientEstimator.ST, epochs=20, \n",
    "                        lr=0.3, label = \"3 hidden layers\")\n",
    "\n",
    "plot_n(res1[1:] + res2[1:] + res3[1:], title=\"Experiment 6: The effect of depth (straight-through)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res1 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.REINFORCE, epochs=20, \n",
    "                       lr=0.05, label = \"1 hidden layer\")\n",
    "res2 = train_classifier(hidden_dims=[100,100], estimator=StochasticGradientEstimator.REINFORCE, epochs=20, \n",
    "                       lr=0.05, label = \"2 hidden layers\")\n",
    "res3 = train_classifier(hidden_dims=[100,100,100], estimator=StochasticGradientEstimator.REINFORCE, epochs=20, \n",
    "                       lr=0.05, label = \"3 hidden layers\")\n",
    "\n",
    "plot_n(res1[1:] + res2[1:] + res3[1:], title=\"Experiment 6: The effect of depth (REINFORCE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res1 = train_classifier(hidden_dims=[100], estimator=StochasticGradientEstimator.ST, epochs=50, \n",
    "                        lr=0.001, label = \"1 hidden layer\")\n",
    "res2 = train_classifier(hidden_dims=[100, 100], estimator=StochasticGradientEstimator.ST, epochs=50, \n",
    "                        lr=0.001, label = \"2 hidden layers\")\n",
    "res3 = train_classifier(hidden_dims=[100, 100, 100], estimator=StochasticGradientEstimator.ST, epochs=50, \n",
    "                        lr=0.001, label = \"3 hidden layers\")\n",
    "\n",
    "plot_n(res1[1:] + res2[1:] + res3[1:], title=\"Experiment 6: The effect of depth (straight-through) (LR = 0.001)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res1 = train_classifier(hidden_dims=[100], epochs=50, non_binary=True, \n",
    "                       lr=0.002, label = \"1 hidden layer\")\n",
    "res2 = train_classifier(hidden_dims=[100,100], epochs=50, non_binary=True, \n",
    "                       lr=0.002, label = \"2 hidden layers\")\n",
    "res3 = train_classifier(hidden_dims=[100,100,100], epochs=50, non_binary=True, \n",
    "                       lr=0.002, label = \"3 hidden layers\")\n",
    "\n",
    "plot_n(res1[1:] + res2[1:] + res3[1:], title=\"Experiment 6: The effect of depth (REINFORCE) (LR = 0.002)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 7: Using binary stochastic neurons as a regularizer.\n",
    "\n",
    "I now test the \"unpublished result\" put forth at the end of [Hinton et al.'s Coursera Lecture 9c](https://www.youtube.com/watch?v=LN0xtUuJsEI&list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9&index=41), which states that we can improve upon the performance of an overfitting multi-layer sigmoid net by turning its neurons binary stochastic neurons with a straight-through estimator. Note that Hinton states that the binary stochastic net might take several times longer to train, which we'll see is true. \n",
    "\n",
    "To test the claim, we will need a dataset that is easier to overfit than MNIST, and so the following experiment uses the MNIST validation set for training (10x smaller than the MNIST training set and therefore much easier to overfit). The hidden layer size is also increased by a factor of 10 to increase overfitting. \n",
    "\n",
    "It took me quite a bit of playing with hyperparameters to get this result, but we can see below that the stochastic net has a clear advantage in terms of the generalization gap and results in a better final fit. Note that 60 epochs are trained below in order to give the stochastic net time to catch up. When testing with a deeper architecture (i.e., 2 hidden layers), I was able to match but not beat the performance of a sigmoidal network with a stochastic one, and this was after 200 epochs of training. \n",
    "\n",
    "Given that I had to run this experiment several times to cook up this result, I see no compelling reason to use binary stochastic neurons over other methods (dropout, weight decay or weight noise) for purposes of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res1 = train_classifier(hidden_dims = [1000], epochs=60, train_set=mnist.validation, val_set=mnist.test, \n",
    "                        lr = 0.03, non_binary = True, label = \"Deterministic sigmoid net\")\n",
    "\n",
    "res2 = train_classifier(hidden_dims = [1000], epochs=60, stochastic_eval=False, train_set=mnist.validation, \n",
    "                        val_set=mnist.test, slope_annealing_rate=1.05, estimator=StochasticGradientEstimator.ST, \n",
    "                        lr = 0.01, label = \"Binary stochastic net\")\n",
    "\n",
    "plot_n(res1 + res2, lower_y=0.8, title=\"Experiment 8: Using binary stochastic neurons as a regularizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this post we introduced, implemented and experimented with binary stochastic neurons in Tensorflow. We saw that with a single binary stochastic layer, the biased straight-through estimator outperforms the unbiased REINFORCE estimator, but that with deeper stochastic architectures, the REINFORCE estimator does better. We explored the variants of each estimator, and showed that the slope-annealed straight through estimator is better than other straight through variants, and that it is worth using the variance-adjusted REINFORCE estimator over the not variance-adjusted REINFORCE estimator. Finally, we explored the potential use for binary stochastic neurons as regularizers, and concluded that it is probably not worth the effort. \n",
    "\n",
    "In future posts, I will look we can take advantage of binary stochastic neurons to make hard decisions within our neural network architecture. This opens the door to conditional computation and some interesting new architectures such as the choice of experts and heirarchichal multiscale RNN architectures. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
